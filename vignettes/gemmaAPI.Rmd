---
title: "Gemma API"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Gemma API}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
# Don't build this whole thing if it's just for a test. Too long running/taxing on Gemma
# if(Sys.getenv('RMD_BUILD') != 1)
#   knitr::knit_exit()

# Prevent certificate issues for GitHub actions
options(gemma.SSL = FALSE)
```

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message=F}
library(gemmaAPI)
library(dplyr)
library(data.table)
library(memoise)
library(microbenchmark)
```

# Getting started

The Gemma API wrapper is very straightforward to use and contains a function for each entry in the REST API docs, [here](https://gemma.msl.ubc.ca/resources/restapidocs/). We suggest you make some example queries on the interactive API docs to get a sense of the input and output.

## Downloading basic information for a dataset

The first endpoint example for the REST API doc corresponds to a simple case: getting basic information about a single dataset in Gemma, such as [GSE2018](https://gemma.msl.ubc.ca/expressionExperiment/showExpressionExperiment.html?id=1). We can do this in one of two ways using the API:

1.  Using the dataset's short name (GSE2018), or
2.  Using the dataset's identifier in Gemma (e.g `1`, `2`, ...)

Each case is functionally identical in the API wrapper.

```{r}
glimpse(getDatasets(datasets = 1))

glimpse(getDatasets(datasets = 'GSE2018'))
```

Many endpoints also allow you to query multiple objects at the same time, for example:

```{r}
glimpse(getDatasets(datasets = c('GSE2018', 'GSE25136')))
```

You'll be able to tell either from the function documentation or simply because the the parameter name is plural (ie. `datasets`, `platforms`).

Some API endpoints only support a single identifier.

```{r error=TRUE}
getDatasetAnnotations(c('GSE2018', 'GSE25136'))
```

In these cases, you will have to loop over all the identifiers you wish to query and send separate requests.

```{r}
lapply(c('GSE2018', 'GSE25136'), function(dataset) {
  getDatasetAnnotations(dataset)[, ee.ShortName := dataset]
}) %>% rbindlist %>% glimpse
```

For large requests, this is often insufficient and instead, we recommend using `async` requests.

## Asynchronous requests

The basic idea is that asynchronous workers can be embedded inside of otherwise synchronous R code by use of a `synchronise` block. Inside this, async requests can be made and the main thread will only block while the synchronise block is being requested.

This is made more obvious by example. Take the previous instance where we wanted to fetch annotations for two datasets, `GSE2018` and `GSE25136`. If we want to run API queries asynchronously, we only need to: 1. Embed our requests inside a `synchronise` block 2. Set `async = TRUE` in our requests

We'll build back up to multiple queries, but starting with a single query, the difference is trivial to see.

```{r}
synchronise({
  getDatasetAnnotations('GSE2018', async = TRUE)
}) %>% glimpse
```

Notice the added `synchronise` block, and that we moved our call to `glimpse` outside of the synchronise block, since the result is guaranteed to be available there.

With multiple queries, there's one additional thing we need to do and that is to actually "unpack" our asynchronous results. We have a few options for this (`when_all`, `when_any`, or `when_some`), but typically we'll just want to receive all the results and thus use `when_all`.

```{r}
synchronise({
  # Make the queries
  res <- lapply(c('GSE2018', 'GSE25136'), function(dataset) {
    getDatasetAnnotations(dataset, async = TRUE)
  })
  
  # Only complete this synchronise block when all queries are finished
  when_all(.list = res)
}) %>% rbindlist %>% glimpse
```

Finally, we'll introduce one last thing you can do with asynchronous results and that is the deferred chain, `$then`. As we've discussed, the result of a Gemma API query with `async = T` is a deferred value. However, it's still possible to work with them when they become available by specifying code to run when that happens. This is particularly useful when you want to compose information from multiple endpoints.

```{r}
synchronise({
  # First, get datasets with more than 100 samples. We'll do this synchronously since we need identifiers to continue
  datasets <- getDatasets(filter = 'numberOfSamples > 100', limit = 10)
  
  # Loop over dataset identifiers that we fetched and get differential expression analyses.
  res <- lapply(datasets$ee.ID, function(dataset) {
    # Get the DEA asynchronously and when it becomes available, run a function on the result
    getDatasetDEA(dataset, async = TRUE)$then(function(response) {
      # Now that we have a result ID, we can get differential expression results for these analyses
      diffEx <- lapply(unique(response$result.ID), function(diffExSet) {
        # Make all these requests asynchronously and add the diffExSet when we receive a response
        getDatasetDE(dataset, diffExSet = diffExSet, async = TRUE)$then(function(response) {
          response[, diffExSet := diffExSet]
        })
      })
      
      # When all of these results become available, bind them together for return
      when_all(.list = diffEx)$then(function(results) rbindlist(results))
    })
  })
  
  # Only complete this synchronise block when all queries are finished
  when_all(.list = res)
}) %>% rbindlist %>% glimpse
```

Although this may look intimidating at first, it's a very powerful way to rapidly make batch queries.

### Benchmark

For completeness, we'll provide a small benchmark to hopefully convince you that asynchronous programming is worth learning. Both of these functions do the same as the above (although one was rewritten to do it synchronously).

```{r}
# First, get datasets with more than 100 samples. We'll do this synchronously since we need identifiers to continue
datasets <- getDatasets(filter = 'numberOfSamples > 100', limit = 10)

asyncBenchmark <- function() {
  synchronise({
    # Loop over dataset identifiers that we fetched and get differential expression analyses.
    res <- lapply(datasets$ee.ID, function(dataset) {
      # Get the DEA asynchronously and when it becomes available, run a function on the result
      getDatasetDEA(dataset, async = TRUE)$then(function(response) {
        # Now that we have a result ID, we can get differential expression results for these analyses
        diffEx <- lapply(unique(response$result.ID), function(diffExSet) {
          # Make all these requests asynchronously and add the diffExSet when we receive a response
          getDatasetDE(dataset, diffExSet = diffExSet, async = TRUE)$then(function(response) {
            response[, diffExSet := diffExSet]
          })
        })
        
        # When all of these results become available, bind them together for return
        when_all(.list = diffEx)$then(function(results) rbindlist(results))
      })
    })
    
    # Only complete this synchronise block when all queries are finished
    when_all(.list = res)
  }) %>% rbindlist
}

syncBenchmark <- function() {
  # Loop over dataset identifiers that we fetched and get differential expression analyses.
  lapply(datasets$ee.ID, function(dataset) {
    # Get the DEA synchronously
    response <- getDatasetDEA(dataset)
    # Now that we have a result ID, we can get differential expression results for these analyses
    lapply(unique(response$result.ID), function(diffExSet) {
      # Make all these requests synchronously and add the diffExSet
      getDatasetDE(dataset, diffExSet = diffExSet)[, diffExSet := diffExSet]
    }) %>% rbindlist
  }) %>% rbindlist
}

knitr::kable(summary(microbenchmark(asyncBenchmark(), syncBenchmark(), times = 10), unit = 's'))
```

As you can see, the asynchronous method is faster by a factor of about 5 (the number of experiments we're querying in this example). For larger queries, the gains are even larger, such as if we wanted to fetch all annotations in Gemma.

```{r}
# For simplicity, we'll assume there are no experiment IDs larger than 20,000.

synchronise({
  # First send a request to an endpoint that accepts multiple datasets to narrow down our search space
  async_map(split(1:20000, ceiling(seq_along(1:20000) / 1000)), async(function(chunk) {
    getDatasets(chunk, limit = 0, async = TRUE)
  }))$then(function(response) {
    # Asynchronously request annotations for each experiment that was found
    async_map(rbindlist(response, fill = TRUE)[, unique(ee.ID)], async(function(dataset) {
      getDatasetAnnotations(dataset, async = TRUE)$then(function(annotations) {
        if(is.data.table(annotations)) annotations[, ee.ID := dataset]
        else NULL
      })
    }), .limit = 200) # Only allow 200 active requests at a time to not flood the server
  })
}) %>% rbindlist %>% glimpse
```

This can (conceivably) be done synchronously, but in practice it would take far too long.

```{r eval=FALSE}
# Not evaluated
response <- lapply(split(1:20000, ceiling(seq_along(1:20000) / 1000)), function(chunk) {
  getDatasets(chunk, limit = 0)
}) %>% rbindlist(fill = TRUE)

lapply(response[, unique(ee.ID)], function(dataset) {
  annotations <- getDatasetAnnotations(dataset)
  if(is.data.table(annotations)) annotations[, ee.ID := dataset]
  else NULL
}) %>% rbindlist %>% glimpse
```

## Memoised results

A different way to "speed up" requests (in a way) is to remember past results so future queries can proceed virtually instantly. This is enabled through the [{memoise}](https://github.com/r-lib/memoise) package. To enable memoisation, simply set `memoised = T`.

```{r}
knitr::kable(summary(microbenchmark(getDatasetData('GSE2018', memoised = TRUE), times = 1), unit = 'ms'))
knitr::kable(summary(microbenchmark(getDatasetData('GSE2018', memoised = TRUE), times = 1), unit = 'ms'))
```

### Clearing cache

If you're done with your fetching and want to ensure no space is being used for cached results, or if you just want to ensure you're getting up-to-date data from Gemma, you can clear the cache using `forgetGemmaMemoised`.

```{r}
forgetGemmaMemoised()
```

## Raw data

By default, Gemma API does some parsing on the raw API results to make it easier to work with inside of R. In the process, it drops some typically unused values. Sometimes, you may wish to to get access to everything, unadulterated by the Gemma API wrapper. This is enabled by setting `raw = T`. Instead of a `data.table`, you'll be served (typically) a list that represents the underlying JSON response.

## File outputs

Sometimes, you may wish to save results to a file for future inspection. You can do this simply by providing a filename to `file`. The extension for this file will be one of three options:

1.  `.json`, if you requested results with `raw = TRUE`
2.  `.csv` if the results have no nested `data.table`s
3.  `.rds` otherwise

You can also specify whether or not the new fetched results are allowed to overwrite an existing file by specifying the `overwrite` parameter.

To illustrate these...

```{r}
glimpse(getDatasetSVD('GSE2018', raw = TRUE, file = 'SVD')) # raw = T, saves SVD.json
glimpse(getDatasetPlatforms('GSE2018', file = 'platforms')) # No nested data.tables, saves platforms.csv
glimpse(getDatasetSamples('GSE2018', file = 'samples')) # Some nested data.tables, saves samples.rds
```

If, for some reason, you absolutely don't want to see the return value showing up in your R instance, you can suppress it using `invisible`.
```{r}
invisible(getDatasetSVD('GSE2018', raw = TRUE, file = 'SVD')) # Saves SVD.json like above, but suppresses output in R
```

```{r include=FALSE}
unlink('SVD.json')
unlink('platforms.csv')
unlink('samples.rds')
```

## Changing defaults

We've seen how to change `async = T`, `memoised = T`, `overwrite = T` and `raw = T`... It's possible that you want to always use the functions these ways without specifying the option every time. You can do this by simply changing the default, which is visible in the function definition. For example, if you want Gemma API to memoise results by default, you can use:

```{r}
options(gemma.memoise = TRUE)
```

## Authentication

If you have an account at [Gemma](https://gemma.msl.ubc.ca) and want to access data that is private, you can login using `setGemmaUser("USERNAME", "PASSWORD")`, providing your actual username and password. These will be passed on to every API request you make until you log out (by calling `setGemmaUser()`).
